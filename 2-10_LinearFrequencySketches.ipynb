{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem\n",
    "\n",
    "Continuous stream coming in. \n",
    "$$ A = \\{a, b, a, a, c, b, a, c, a, d, d, d\\} $$\n",
    "\n",
    "We want to find the occurrence of each element using Count-Min Sketch.\n",
    "\n",
    "# Count-Min Sketch\n",
    "\n",
    "Two hash functions.\n",
    "\n",
    "$$h_1 = \\{(a, 1), (b, 2), (c, 2), (d, 1)\\}$$\n",
    "$$h_2 = \\{(a, 2), (b, 1), (c, 1), (d, 1)\\}$$\n",
    "\n",
    "Then the table is:\n",
    "\n",
    "|       | m=1 | m=2 |\n",
    "|-------|---|---|\n",
    "| $h_1$ | 8 | 4 |\n",
    "| $h_2$ | 7 | 5 |\n",
    "\n",
    "Thus, the estimated occurrence of each element is\n",
    "\n",
    "$$ \\hat{a} = \\min(8, 5) = 5 = 5 $$\n",
    "$$ \\hat{b} = \\min(4, 7) = 4 \\neq 2 $$\n",
    "$$ \\hat{c} = \\min(4, 7) = 4 \\neq 2 $$\n",
    "$$ \\hat{d} = \\min(8, 7) = 7 \\neq 3 $$\n",
    "\n",
    "The problem of Count-Min Sketch is that (on average) expected error is ok but actual performance is quite bad.\n",
    "The expected count of $i$-th element of particular hash function is\n",
    "\n",
    "$$ E[X_i] = |a_i| + \\frac{|A| - |a_i|}{m} $$\n",
    "\n",
    "where $|a_i|$ is the number of an element got hashed to the $i$-th bucket.\n",
    "The expected error for $|a_i|$ is $\\frac{|A| - |a_i|}{m}$.\n",
    "\n",
    "Using $d$ hash functions, we model the error as \n",
    "\n",
    "$$E[error] = \\frac{|A| - |a_i|}{m d}$$\n",
    "\n",
    "Any low-frequency elements that collide with high-frequency elements will have over-reported frequency.\n",
    "\n",
    "\n",
    "[https://web.stanford.edu/class/archive/cs/cs166/cs166.1166/lectures/11/Slides11.pdf]\n",
    "[https://web.stanford.edu/class/cs166/]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Frequency Sketches\n",
    "\n",
    "Count-Min Sketch could be modelled as a special case of Linear Frequency Sketches.\n",
    "\n",
    "For example, the $h_1$ can be expressed as a matrix\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 1 \\\\\n",
    "0 & 1 & 1 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If you multiply the matrix with the vector of the occurrence of each element, you get the row corresponding to the hash function $h_1$.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 1 \\\\\n",
    "0 & 1 & 1 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "5 \\\\\n",
    "2 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "8 \\\\\n",
    "4 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$h_2$ can be expressed as a matrix\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 1 & 1 \\\\\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Therefore, the Count-Min Sketch can be obtained by multiplying $md \\times n$ matrix with $n \\times 1$ vector.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 1 \\\\\n",
    "0 & 1 & 1 & 0 \\\\\n",
    "0 & 1 & 1 & 1 \\\\\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "5 \\\\\n",
    "2 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "8 \\\\\n",
    "4 \\\\\n",
    "7 \\\\\n",
    "5 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Usually, the number of categories is much larger than $md$. Therefore, we can see this as randomized dimensionality reduction. Note that Johnson-Lindenstrauss Lemma indicated that using randomized matrix, we can reduce the dimensionality without losing much information.\n",
    "\n",
    "$$\n",
    "\\hat{f} = G  f\n",
    "$$\n",
    "where\n",
    "True frequency vector $f \\in \\mathbb{R}^{N}$, $N$ is the number of categories.\n",
    "$$\n",
    "f \\in \\mathbb{R}^{N}\n",
    "$$\n",
    "G is the sketching matrix\n",
    "$$\n",
    "G \\in \\mathbb{R}^{md \\times N}\n",
    "$$\n",
    "Sketch vector $\\hat{f} \\in \\mathbb{R}^{md}$.\n",
    "$$\n",
    "\\hat{f} \\in \\mathbb{R}^{md}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histograms are also Linear Frequency Sketches\n",
    "\n",
    "Histograms are also Linear Frequency Sketches.\n",
    "\n",
    "$G$ for histogram looks like:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "# Linearity of Linear Frequency Sketches\n",
    "\n",
    "We can add two histograms.\n",
    "\n",
    "$$\n",
    "\\hat{f} = G (f_1 + f_2) = G f_1 + G f_2 = \\hat{f}_1 + \\hat{f}_2\n",
    "$$\n",
    "\n",
    "# Answering Count Queries from $\\hat{f}$\n",
    "\n",
    "We can answer the count queries from $\\hat{f}$.\n",
    "\n",
    "$$\n",
    "\\hat{f} = G f\n",
    "$$\n",
    "\n",
    "$$\n",
    "f = G^{-1} \\hat{f}\n",
    "$$\n",
    "\n",
    "If $G$ is invertible, we can answer the count queries from $\\hat{f}$.\n",
    "We can always make $G$ invertible by adding regularization term.\n",
    "\n",
    "To find $f$, we solve\n",
    "\n",
    "$$\n",
    "\\min_f ||G f - \\hat{f}||^2 + \\lambda ||f||^2\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is the regularization term. (Ridge regression)\n",
    "\n",
    "Then \n",
    "$$\n",
    "f = (G^T G + \\lambda I)^{-1} G^T \\hat{f}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does this work?\n",
    "1) Power of randomized dimensionality reduction.\n",
    "2) Distances between Distribution are preserved in the lower dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why does Randomized Dimensionality Reduction Work?\n",
    "\n",
    "* Concentration of measure\n",
    "\n",
    "If we have high dimensional data, distances become smaller after linear projection into lower dimensional space.\n",
    "Imagine we project 3D data into 2D space. The distance between two points in 3D space becomes smaller in 2D space.\n",
    "\n",
    "* Preservation of inner product\n",
    "\n",
    "$$\n",
    "\\langle G x, G y \\rangle = x^T G^T G y\n",
    "$$\n",
    "$$\n",
    "E[x^T G^T G x] = x^T E[G^T G] x = x^T m I x = m ||x||^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distances between Distribution are preserved in the lower dimension\n",
    "\n",
    "* Kullback-Leibler divergence\n",
    "* Hellinger distance\n",
    "\n",
    "These distances are preserved in the lower dimension.\n",
    "\n",
    "See also: DistancesBetweenDistributions.ipynb"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
