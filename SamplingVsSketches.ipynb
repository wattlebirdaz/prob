{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Number of Elements in a Set\n",
    "\n",
    "You are tasked with counting how many times a particular value $a$ shows up in a set $A = \\{a_1, a_2, \\ldots, a_n\\}$.\n",
    "\n",
    "There are two main algorithms proposed to solve this problem:\n",
    "- Algorithm 1: Space complexity $O(N)$, Time complexity $O(N)$. This method involves iterating through each element in the set and counting occurrences of $a$ directly.\n",
    "- Algorithm 2: Space complexity $O(\\text{distinct elements})$, Time complexity $O(1)$. This approach pre-processes the set to store the count of each distinct element in a hash map, allowing for $O(1)$ lookup time for the count of any element.\n",
    "\n",
    "# Q. How can we improve space complexity in sacrifice of accuracy?\n",
    "\n",
    "## Naive Solution\n",
    "\n",
    "The naive solution involves sampling $k$ elements from the set $A$. Let $k'$ denote the count of $a$ within the sampled elements. The estimated total count of $a$ in the set is then given by $\\hat{N} = N \\cdot \\frac{k'}{k}$, where $N$ is the total number of elements in the set.\n",
    "\n",
    "## Error Analysis\n",
    "\n",
    "### How far away is $\\hat{N}$ from the true count $|a|$?\n",
    "\n",
    "- The error in the estimation can be considered as stemming from the variance of a Bernoulli process, as each sample is a Bernoulli trial with success if the sampled element is $a$.\n",
    "- The standard error of the proportion (in this case, $\\frac{k'}{k}$) can be given by the square root of the Bernoulli variance, which is $\\sqrt{\\frac{p(1-p)}{k}}$, where $p$ is the true proportion of $a$ in the set. Therefore, the error in $\\hat{N}$ can be roughly estimated as $O\\left(\\frac{1}{\\sqrt{k}}\\right)$.\n",
    "\n",
    "### Comparison to Histogram Error\n",
    "\n",
    "- In a histogram-based approach, the error reduces at a rate of $O\\left(\\frac{1}{k}\\right)$. This difference arises because, in the naive sampling method, each sample is independent, leading to a $\\sqrt{k}$ rate of convergence due to the Central Limit Theorem.\n",
    "\n",
    "## Improving Error Convergence\n",
    "\n",
    "To improve the rate at which the error converges, one could consider:\n",
    "- **Increasing the sample size ($k$)**: This directly reduces the error but does so only at the rate of $\\frac{1}{\\sqrt{k}}$.\n",
    "- **Stratified Sampling**: If some prior knowledge about the distribution of elements in $A$ is available, stratified sampling can ensure that the sample more accurately reflects the overall population, potentially reducing variance.\n",
    "- **Advanced Estimation Techniques**: Techniques such as bootstrapping or Bayesian estimation could provide better error characteristics under certain conditions, especially if prior information about the distribution of $A$ can be leveraged.\n",
    "\n",
    "In summary, while the naive solution provides a straightforward means to estimate the count of $a$ in $A$, its accuracy is inherently limited by the statistical properties of sampling. Advanced techniques and larger sample sizes can improve estimation accuracy but may also come with increased computational or space complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prove that the expected error of the estimation $\\hat{N} = N\\frac{k'}{k}$, where $k'$ is the observed count of $a$ in a sample of size $k$ from a set of size $N$, is $O\\left(\\frac{1}{\\sqrt{k}}\\right)$, we need to clarify the notation and work through the derivation step by step.\n",
    "\n",
    "### Setup\n",
    "- Let $X_i$ be a random variable that equals 1 if the $i$-th sampled element is $a$, and 0 otherwise. Thus, $X_i \\sim \\text{Bernoulli}(p)$, where $p = \\frac{|a|}{N}$ is the true proportion of $a$ in the set $A$.\n",
    "- The count $k'$ can be expressed as $k' = \\sum_{i=1}^{k} X_i$.\n",
    "- The estimator $\\hat{N} = N\\frac{k'}{k}$ estimates the total count of $a$ in the set.\n",
    "\n",
    "### Goal\n",
    "We want to prove that $\\mathbb{E}\\left[\\left(\\hat{N} - |a|\\right)^2\\right] = O\\left(\\frac{1}{k}\\right)$. Hence, $\\mathbb{E}\\left[|\\hat{N} - |a||\\right] = O\\left(\\frac{1}{\\sqrt{k}}\\right)$\n",
    "\n",
    "### Proof\n",
    "\n",
    "1. **Variance of $k'$**: Since $k'$ is the sum of $k$ independent Bernoulli trials, each with variance $\\sigma^2 = p(1-p)$, the variance of $k'$ is $k\\sigma^2 = kp(1-p)$.\n",
    "\n",
    "2. **Expected Value of $\\hat{N}$**: \n",
    "   $$\n",
    "   \\mathbb{E}[\\hat{N}] = \\mathbb{E}\\left[N\\frac{k'}{k}\\right] = N\\frac{\\mathbb{E}[k']}{k} = N\\frac{kp}{k} = Np = |a|\n",
    "   $$\n",
    "   This shows that $\\hat{N}$ is an unbiased estimator of $|a|$.\n",
    "\n",
    "3. **Variance of $\\hat{N}$**: \n",
    "   $$\n",
    "   \\text{Var}(\\hat{N}) = \\text{Var}\\left(N\\frac{k'}{k}\\right) = N^2\\frac{\\text{Var}(k')}{k^2} = N^2\\frac{kp(1-p)}{k^2} = \\frac{N^2p(1-p)}{k}\n",
    "   $$\n",
    "\n",
    "4. **Expected Squared Error**: ($bias^2 + variance$)\n",
    "   $$\n",
    "   \\mathbb{E}\\left[\\left(\\hat{N} - |a|\\right)^2\\right] = \\text{Var}(\\hat{N}) = \\frac{N^2p(1-p)}{k}\n",
    "   $$\n",
    "   Since $p = \\frac{|a|}{N}$, this simplifies to:\n",
    "   $$\n",
    "   \\frac{N^2\\frac{|a|}{N}\\left(1-\\frac{|a|}{N}\\right)}{k} = \\frac{|a|(N-|a|)}{k}\n",
    "   $$\n",
    "\n",
    "5. **Rate of Convergence**: The expected squared error $\\frac{|a|(N-|a|)}{k}$ simplifies to $O\\left(\\frac{1}{k}\\right)$. Therefore, the expected error simplifies to $O\\left(\\frac{1}{\\sqrt{k}}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  In a histogram-based approach, the error reduces at a rate of $O\\left(\\frac{1}{k}\\right)$. This difference arises because, in the naive sampling method, each sample is independent, leading to a $\\sqrt{k}$ rate of convergence due to the Central Limit Theorem.\n",
    "\n",
    "The statement refers to two different methods of estimating a parameter (like the frequency of an element in a dataset) and how the error in these estimations decreases as the number of samples increases. This involves understanding how errors behave in simple random sampling versus more complex estimation methods, and where the Central Limit Theorem (CLT) comes into play.\n",
    "\n",
    "### Histogram-Based Approach and Error Reduction\n",
    "\n",
    "In a histogram-based approach to estimating the frequency of an element, you essentially use the entire dataset to construct the histogram. When the statement mentions the error reduces at a rate of $O\\left(\\frac{1}{k}\\right)$, it seems to conflate the histogram approach with methods that might average multiple estimates or rely on large data sets to directly calculate frequencies, where $k$ is the size of the dataset or the number of observations used directly in the estimation.\n",
    "\n",
    "However, typically, in histogram approaches, the error isn't necessarily described by $O\\left(\\frac{1}{k}\\right)$ because you're not sampling; you're using full data. The error discussion more accurately applies to estimation processes involving sampling or averaging multiple estimates from subsets of data.\n",
    "\n",
    "### Naive Sampling Method and CLT\n",
    "\n",
    "In the context of the naive sampling method, you're drawing a sample of size $k$ from the dataset and using the results from this sample to estimate the frequency of an element. Here's where the Central Limit Theorem (CLT) becomes crucial:\n",
    "\n",
    "- **Central Limit Theorem (CLT):** The CLT states that the sampling distribution of the sample mean (or sum) will approach a normal distribution as the sample size $k$ becomes larger, regardless of the shape of the original distribution, provided the samples are independent and identically distributed (i.i.d.).\n",
    "\n",
    "- **Application to Error Rate:** When estimating the frequency of an element via sampling, the error of the estimate depends on the variance of the sampling distribution. According to the CLT, as $k$ increases, the standard error (which is the standard deviation of the sampling distribution of the estimate) decreases. Specifically, the standard error of the mean (or proportion, in the case of frequency estimation) is proportional to $\\frac{1}{\\sqrt{k}}$, where $k$ is the sample size. This is because the variance of the sum (or count) of successes in $k$ Bernoulli trials is $kp(1-p)$, leading to a standard deviation proportional to $\\sqrt{k}$, and hence the standard error of the proportion is proportional to $\\frac{1}{\\sqrt{k}}$.\n",
    "\n",
    "### Comparing Error Rates\n",
    "\n",
    "The statement contrasts the error rates between a histogram-based approach and a naive sampling method, suggesting that the naive method's error decreases at a $\\sqrt{k}$ rate due to the CLT. The critical point here is understanding that the CLT informs us about the behavior of the distribution of sample averages (or proportions) as the sample size increases, not directly about the \"error\" in a conventional sense. The $O\\left(\\frac{1}{\\sqrt{k}}\\right)$ term specifically refers to the rate at which the standard error of the estimate decreases as the sample size increases, a direct consequence of the CLT.\n",
    "\n",
    "In summary, the CLT plays a pivotal role in understanding why, in sampling methods, the precision of estimates improves at a rate of $O\\left(\\frac{1}{\\sqrt{k}}\\right)$ with increasing sample size, due to the behavior of the variance of the sampling distribution. This is different from direct calculations or more deterministic approaches (like histograms), where the concept of sampling error and its reduction doesn't apply in the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Let's elaborate on how the error of estimating the mean (or proportion) of an element in a set decreases with increasing sample size using the Central Limit Theorem (CLT) and mathematical equations. We'll consider a generic scenario where you're estimating the mean or proportion from a sample.\n",
    "\n",
    "### Setup for Estimating a Mean or Proportion\n",
    "\n",
    "Assume you have a population with a true mean $\\mu$ and a true standard deviation $\\sigma$. You're interested in estimating the mean of this population by taking samples of size $n$. Each sample provides an estimate of the population mean, which we denote as $\\bar{x}$ for a specific sample.\n",
    "\n",
    "### Central Limit Theorem (CLT)\n",
    "\n",
    "The Central Limit Theorem states that, given a sufficiently large sample size $n$, the sampling distribution of the sample mean $\\bar{x}$ will be normally distributed, or approximately normal if the population distribution is not normal, with mean $\\mu$ (the same as the population mean) and standard deviation $\\frac{\\sigma}{\\sqrt{n}}$ (known as the standard error of the mean), regardless of the population's distribution. Mathematically, this is expressed as:\n",
    "\n",
    "$$\n",
    "\\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\n",
    "$$\n",
    "\n",
    "### Error in Estimating the Mean\n",
    "\n",
    "The error in the estimate of the mean can be quantified by the standard error (SE), which measures the dispersion of the sampling distribution of the sample mean. The standard error of the mean is given by:\n",
    "\n",
    "$$\n",
    "SE = \\frac{\\sigma}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "This equation shows that the precision of our estimate increases (i.e., the error decreases) as the sample size $n$ increases.\n",
    "\n",
    "\n",
    "### Implication of CLT\n",
    "\n",
    "The implication of the CLT for estimating means (or proportions, by treating the proportion as a mean of a Bernoulli distribution with success probability $p$ and $\\sigma = \\sqrt{p(1-p)}$) is profound. It tells us that by increasing the sample size $n$, we can make the sampling distribution of our estimator (the sample mean $\\bar{x}$) more concentrated around the true mean $\\mu$, reducing the standard error and thus the uncertainty of our estimate.\n",
    "\n",
    "### Summary\n",
    "\n",
    "In summary, the CLT provides a powerful foundation for understanding how the error in estimating the mean or proportion decreases with the square root of the sample size. It highlights the importance of sample size in statistical estimation and the practical approach to increasing estimation accuracy through larger samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When estimating the proportion of elements (for instance, the proportion of times a specific element appears in a dataset), the setup is quite similar to estimating a mean, but with a focus on binary outcomes. This scenario often arises in contexts like polling, where you might want to know the proportion of the population that favors a certain option. The Central Limit Theorem (CLT) similarly applies to proportions, offering insights into how the error in estimating a proportion decreases as the sample size increases.\n",
    "\n",
    "### Setup for Estimating a Proportion\n",
    "\n",
    "Assume you have a population where a proportion $p$ of the population has a certain characteristic (e.g., choosing a specific answer in a poll). You sample $n$ individuals from this population and observe the proportion $\\hat{p}$ of individuals in your sample with this characteristic.\n",
    "\n",
    "### Central Limit Theorem (CLT) for Proportions\n",
    "\n",
    "The CLT states that, for a large enough sample size $n$, the sampling distribution of the sample proportion $\\hat{p}$ will approximate a normal distribution with mean $p$ (the true population proportion) and standard deviation $\\sqrt{\\frac{p(1-p)}{n}}$, which is the standard error of the proportion. Mathematically, this is given by:\n",
    "\n",
    "$$\n",
    "\\hat{p} \\sim N\\left(p, \\sqrt{\\frac{p(1-p)}{n}}\\right)\n",
    "$$\n",
    "\n",
    "### Error in Estimating the Proportion\n",
    "\n",
    "The error in the estimate of the proportion can be quantified by the standard error (SE), which measures the dispersion of the sampling distribution of the sample proportion. The standard error of the proportion is:\n",
    "\n",
    "$$\n",
    "SE(\\hat{p}) = \\sqrt{\\frac{p(1-p)}{n}}\n",
    "$$\n",
    "\n",
    "This equation shows that, similar to estimating a mean, the precision of our estimate of a proportion increases (i.e., the error decreases) as the sample size $n$ increases.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The application of the CLT to estimating proportions reveals that as the sample size $n$ increases, the standard error of the proportion decreases, leading to more precise estimates. This decrease in error follows the $\\sqrt{n}$ pattern, identical to that observed when estimating means. Thus, for large samples, the sampling distribution of both means and proportions will tend to normality, facilitating the application of normal distribution properties to construct confidence intervals and conduct hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "* If you sample $k$ rows from the dataset, and compute the prediction from those samples, the difference between the prediction and the true value becomes proportional to $1/\\sqrt{k}$.\n",
    "* Histogram uses all the rows in the dataset. However, it does not have space to store all the information of the dataset. It only has space for $k$ bins. Thus, even though we see all the dataset, the query result contains error compared with the true value. The difference between then the prediction and the true value (bias) is proportional to $1/k$.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
